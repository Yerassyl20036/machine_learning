# Техническая работа: Реализация линейной и логистической регрессии с нуля

## Оглавление

1. [Резюме](#1-резюме)
2. [Введение](#2-введение)
3. [Пайплайн данных и предобработка](#3-пайплайн-данных-и-предобработка)
4. [Математическое обоснование](#4-математическое-обоснование)
5. [Детали реализации](#5-детали-реализации)
6. [Результаты и анализ](#6-результаты-и-анализ)
7. [Клинический отчёт](#7-клинический-отчёт)
8. [Выводы](#8-выводы)
9. [Список литературы](#9-список-литературы)

---

## 1. Резюме

### Цели работы

Реализовать алгоритмы линейной и логистической регрессии с нуля (без использования высокоуровневых функций scikit-learn), применить их к задаче классификации геномных вариантов по клинической значимости и провести сравнительный анализ с эталонными реализациями scikit-learn.

### Набор данных

- **Источник**: синтетический набор данных, сгенерированный скриптом `src/download_genomics_data.py` (имитация формата ClinVar)
- **Исходный размер**: 2 000 записей
- **После фильтрации** (удалены VUS — варианты неопределённой значимости): **1 606 записей**
- **Признаки**: chromosome, position, gene, REF, ALT, genotype, depth, effect_type, clinical_significance
- **Целевая переменная**: бинарная метка — Pathogenic (1) vs Benign (0)

### Основные результаты

| Метрика | Собственная реализация | scikit-learn |
|---------|----------------------|-------------|
| Accuracy (логист.) | 0.4938 | 0.4938 |
| F1-Score (логист.) | 0.5534 | 0.5534 |
| ROC AUC (логист.) | 0.5334 | 0.5334 |
| MSE (линейная) | 20 734.61 | 20 734.61 |
| R² (линейная) | −0.0089 | −0.0089 |

> **Примечание.** Синтетические данные намеренно случайны (нет реальной связи между признаками и меткой), поэтому точность обеих моделей близка к 50 %. Главная цель — продемонстрировать корректность реализации: совпадение метрик с scikit-learn подтверждает правильность алгоритмов.

### Ключевые выводы

- Собственные реализации дают идентичные результаты с scikit-learn, что подтверждает корректность градиентного спуска и вычисления функций потерь.
- Без масштабирования признаков градиентный спуск расходится из-за больших значений позиции (до 250 млн); StandardScaler решает эту проблему.
- Анализ сходимости показывает, что скорость обучения 0.01 является оптимальной для данного набора данных.

---

## 2. Введение

### Цели задания

1. Реализовать линейную регрессию с нуля с использованием аналитического вычисления градиентов и MSE-функции потерь.
2. Реализовать логистическую регрессию с нуля с сигмоидной функцией активации и бинарной кросс-энтропией.
3. Сравнить собственные реализации с scikit-learn.
4. Провести анализ сходимости для различных скоростей обучения.
5. Применить модели к задаче классификации геномных вариантов.

### Постановка задачи

В геномике вариантный анализ — ключевой этап определения клинической значимости генетических мутаций. Необходимо по набору признаков варианта (хромосома, позиция, ген, аллели, глубина покрытия, тип эффекта) предсказать, является ли вариант **патогенным** (1) или **доброкачественным** (0).

### Описание набора данных

**Источник**: синтетический генератор (`src/download_genomics_data.py`), альтернативно — ClinVar VCF (NCBI FTP).

**Размер**: 2 000 исходных записей → 1 606 после удаления VUS.

**Признаки**:

| Признак | Описание | Тип |
|---------|----------|-----|
| chromosome | Хромосома (1–22, X, Y, MT) | Категориальный |
| position | Геномная позиция варианта | Числовой |
| gene | Название гена (BRCA1, TP53, EGFR и др.) | Категориальный |
| REF | Референсный аллель | Категориальный |
| ALT | Альтернативный аллель | Категориальный |
| genotype | Генотип (0/0, 0/1, 1/1, 1/2) | Категориальный |
| depth | Глубина покрытия (прочтения) | Числовой |
| effect_type | Тип эффекта мутации | Категориальный |
| clinical_significance | Клиническая значимость | Категориальный (целевой) |

**Целевая переменная**: `label` — бинаризованная клиническая значимость:
- 0 = benign, likely_benign
- 1 = pathogenic, likely_pathogenic
- VUS (uncertain_significance) — исключены

### Пояснение геномных терминов

- **REF (референсный аллель)** — нуклеотид в референсном геноме на данной позиции (A, C, G или T).
- **ALT (альтернативный аллель)** — нуклеотид, обнаруженный у пациента, отличающийся от референса.
- **Генотипы (GT)**:
  - `0/0` — гомозигота по референсу (мутации нет)
  - `0/1` — гетерозигота (одна копия мутации)
  - `1/1` — гомозигота по альтернативному аллелю (обе копии мутантные)
  - `1/2` — компаунд-гетерозигота (два различных альтернативных аллеля)
- **Типы эффектов мутации**:
  - `missense_variant` — замена аминокислоты
  - `frameshift_variant` — сдвиг рамки считывания
  - `stop_gained` — преждевременный стоп-кодон
  - `synonymous_variant` — синонимичная замена (аминокислота не меняется)
  - `splice_region_variant` — изменение сайта сплайсинга
  - `intron_variant` — вариант в интроне
- **Клиническая значимость**: benign, likely_benign, uncertain_significance (VUS), likely_pathogenic, pathogenic

---

## 3. Пайплайн данных и предобработка

### Шаг A: Контроль качества данных (QC)

**Загружено вариантов**: 2 000

**Статистика покрытия (глубина, depth)**:

| Показатель | Значение |
|-----------|----------|
| Количество | 1 606 |
| Среднее | 252.71 |
| Стандартное отклонение | 143.68 |
| Минимум | 5 |
| 25-й перцентиль | 126.25 |
| Медиана | 249.00 |
| 75-й перцентиль | 379.00 |
| Максимум | 500 |

**Анализ пропущенных значений**: пропущенные значения заполнены медианой (для depth) или меткой «unknown» (для категориальных признаков).

**Распределение типов вариантов** (в исходных данных):

| Тип эффекта | Количество |
|-------------|-----------|
| missense_variant | 348 |
| intron_variant | 335 |
| stop_gained | 332 |
| synonymous_variant | 331 |
| splice_region_variant | 330 |
| frameshift_variant | 324 |

**Распределение клинической значимости** (в исходных данных):

| Значимость | Количество |
|-----------|-----------|
| likely_pathogenic | 427 |
| likely_benign | 400 |
| uncertain_significance | 394 |
| benign | 393 |
| pathogenic | 386 |

**Распределение меток после фильтрации**:
- Pathogenic (1): 813
- Benign (0): 793

Классы сбалансированы (соотношение ≈ 1.03:1).

### Шаг B: Контекст выравнивания

- **Референсный геном**: GRCh38 (hg38) — последняя сборка генома человека.
- **Процесс**: FASTQ → выравнивание (BWA-MEM) → BAM → вызов вариантов (GATK HaplotypeCaller) → VCF.
- В данном проекте мы начинаем с готового VCF-файла или синтетических данных.

### Шаг C: Результаты вызова вариантов

**Распределение генотипов**:

| Генотип | Количество |
|---------|-----------|
| 1/1 | 422 |
| 0/1 | 404 |
| 1/2 | 399 |
| 0/0 | 381 |

**Топ-5 хромосом по количеству вариантов**:

| Хромосома | Количество |
|-----------|-----------|
| 16 | 77 |
| 18 | 74 |
| 11 | 73 |
| MT | 73 |
| 5 | 73 |

**Примеры вариантов (первые 10 строк)**:

| chromosome | position | gene | REF | ALT | genotype | depth | effect_type | clinical_significance | label |
|-----------|----------|------|-----|-----|----------|-------|-------------|----------------------|-------|
| 11 | 20024031 | APOE | T | C | 0/1 | 338 | frameshift_variant | likely_pathogenic | 1 |
| 16 | 178062986 | KRAS | G | C | 1/2 | 184 | splice_region_variant | pathogenic | 1 |
| 3 | 88810260 | APOE | T | C | 1/2 | 252 | stop_gained | likely_pathogenic | 1 |
| 13 | 43264903 | MYC | C | T | 1/2 | 131 | intron_variant | likely_pathogenic | 1 |
| 20 | 125363412 | APOE | C | G | 0/1 | 192 | synonymous_variant | likely_benign | 0 |
| 22 | 198660172 | TP53 | G | A | 0/0 | 260 | stop_gained | pathogenic | 1 |
| Y | 58475064 | CFTR | T | G | 1/2 | 243 | frameshift_variant | likely_benign | 0 |
| 12 | 220201919 | BRCA1 | A | T | 1/2 | 353 | missense_variant | likely_pathogenic | 1 |
| 16 | 93056405 | EGFR | T | C | 0/0 | 391 | missense_variant | benign | 0 |
| 12 | 46967242 | MYC | A | G | 0/1 | 45 | stop_gained | pathogenic | 1 |

### Шаг D: Фильтрация и нормализация

**Критерии фильтрации**:
1. Удалены варианты с `clinical_significance = uncertain_significance` (VUS) — 394 записи.
2. Удалены записи с пустыми chromosome, position, REF, ALT.
3. Итого после фильтрации: **1 606 записей** (из 2 000).

**Обработка пропущенных значений**:
- `depth`: заполнены медианой
- `gene`, `genotype`, `effect_type`: заполнены меткой «unknown»

**Кодирование категориальных признаков**:
- `effect_type` → one-hot encoding (6 столбцов: `effect_frameshift_variant`, `effect_intron_variant`, `effect_missense_variant`, `effect_splice_region_variant`, `effect_stop_gained`, `effect_synonymous_variant`)
- `clinical_significance` → one-hot encoding (4 столбца: `clinsig_benign`, `clinsig_likely_benign`, `clinsig_likely_pathogenic`, `clinsig_pathogenic`)

**Масштабирование признаков**: StandardScaler (z-нормализация) перед обучением, так как `position` имеет диапазон [1, 250 000 000], а `depth` — [5, 500].

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

### Шаг E: Аннотация признаков

**Итоговый набор признаков (20 столбцов)**:

| Категория | Признаки |
|----------|----------|
| Числовые | `position`, `depth` |
| One-hot (эффект) | `effect_frameshift_variant`, `effect_intron_variant`, `effect_missense_variant`, `effect_splice_region_variant`, `effect_stop_gained`, `effect_synonymous_variant` |
| One-hot (значимость) | `clinsig_benign`, `clinsig_likely_benign`, `clinsig_likely_pathogenic`, `clinsig_pathogenic` |

> **Примечание**: столбцы `clinsig_*` напрямую кодируют целевую переменную, что в реальном сценарии создаёт утечку данных. В данной учебной работе это допускается для демонстрации алгоритмов.

---

## 4. Математическое обоснование

### Теория линейной регрессии

**Гипотеза (модель)**:

$$h(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + b = w_1 x_1 + w_2 x_2 + \ldots + w_n x_n + b$$

**Функция потерь (MSE)**:

$$L(\mathbf{w}, b) = \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2$$

где $\hat{y}_i = \mathbf{w}^T \mathbf{x}_i + b$.

**Вычисление градиентов**:

Раскроем функцию потерь:

$$L = \frac{1}{n} \sum_{i=1}^{n} (\mathbf{w}^T \mathbf{x}_i + b - y_i)^2$$

Производная по вектору весов $\mathbf{w}$:

$$\frac{\partial L}{\partial \mathbf{w}} = \frac{2}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i) \cdot \mathbf{x}_i = \frac{2}{n} \mathbf{X}^T (\hat{\mathbf{y}} - \mathbf{y})$$

Производная по смещению $b$:

$$\frac{\partial L}{\partial b} = \frac{2}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)$$

**Правило обновления (градиентный спуск)**:

$$\mathbf{w} \leftarrow \mathbf{w} - \alpha \cdot \frac{\partial L}{\partial \mathbf{w}}$$
$$b \leftarrow b - \alpha \cdot \frac{\partial L}{\partial b}$$

где $\alpha$ — скорость обучения (learning rate).

Реализация в коде:

```python
# Аналитические градиенты для MSE
grad_w = (2.0 / n_samples) * (X.T @ error)
grad_b = (2.0 / n_samples) * float(np.sum(error))

self.weights -= self.lr * grad_w
self.bias -= self.lr * grad_b
```

### Теория логистической регрессии

**Сигмоидная функция**:

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

Свойства: $\sigma(z) \in (0, 1)$, $\sigma'(z) = \sigma(z)(1 - \sigma(z))$.

**Гипотеза**:

$$h(\mathbf{x}) = \sigma(\mathbf{w}^T \mathbf{x} + b)$$

Значение $h(\mathbf{x})$ интерпретируется как вероятность принадлежности к классу 1: $P(y=1 | \mathbf{x})$.

**Функция потерь (бинарная кросс-энтропия, BCE)**:

$$L(\mathbf{w}, b) = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]$$

где $p_i = \sigma(\mathbf{w}^T \mathbf{x}_i + b)$.

**Вывод градиента по $\mathbf{w}$**:

Шаг 1. Рассмотрим вклад одного образца:

$$\ell_i = -y_i \log(p_i) - (1 - y_i) \log(1 - p_i)$$

Шаг 2. Производная $\ell_i$ по $z_i = \mathbf{w}^T \mathbf{x}_i + b$:

$$\frac{\partial \ell_i}{\partial z_i} = -y_i \cdot \frac{1}{p_i} \cdot p_i(1-p_i) + (1-y_i) \cdot \frac{1}{1-p_i} \cdot p_i(1-p_i)$$

$$= -y_i(1 - p_i) + (1-y_i) p_i = p_i - y_i$$

Шаг 3. Применяем цепное правило:

$$\frac{\partial \ell_i}{\partial \mathbf{w}} = (p_i - y_i) \cdot \mathbf{x}_i$$

Шаг 4. Суммируем по всем образцам:

$$\frac{\partial L}{\partial \mathbf{w}} = \frac{1}{n} \sum_{i=1}^{n} (p_i - y_i) \cdot \mathbf{x}_i = \frac{1}{n} \mathbf{X}^T (\mathbf{p} - \mathbf{y})$$

$$\frac{\partial L}{\partial b} = \frac{1}{n} \sum_{i=1}^{n} (p_i - y_i)$$

Реализация в коде:

```python
# Аналитические градиенты для BCE
error = probs - y
grad_w = (1.0 / n_samples) * (X.T @ error)
grad_b = (1.0 / n_samples) * float(np.sum(error))

self.weights -= self.lr * grad_w
self.bias -= self.lr * grad_b
```

> **Замечание**: формы градиентов для линейной и логистической регрессии совпадают по структуре, хотя функции потерь различны. В логистической регрессии «error» = $p - y$ (вероятность минус метка), а в линейной — $\hat{y} - y$ (предсказание минус истинное значение).

---

## 5. Детали реализации

### Архитектура кода

Проект организован следующим образом:

```
ML_masters_degree/
├── main.py                    # Оркестратор пайплайна
├── requirements.txt
├── config/
│   └── db_config.json         # Конфигурация БД
├── data/
│   ├── raw/                   # Сырые данные
│   └── processed/             # Обработанные данные
├── src/
│   ├── __init__.py
│   ├── db_setup.py            # Настройка БД (PostgreSQL/SQLite)
│   ├── download_genomics_data.py  # Загрузка/генерация данных
│   ├── preprocess_genomics.py     # Предобработка
│   ├── linear_regression_scratch.py   # Линейная регрессия
│   ├── logistic_regression_scratch.py # Логистическая регрессия
│   ├── compare_models.py         # Сравнение моделей
│   └── convergence_analysis.py   # Анализ сходимости
├── notebooks/
│   └── analysis.ipynb         # Jupyter-ноутбук
└── results/                   # Результаты и графики
```

### Класс `LinearRegressionScratch`

```python
class LinearRegressionScratch:
    def __init__(self, lr=0.01, n_iters=1000):
        self.lr = lr
        self.n_iters = n_iters
        self.weights = None
        self.bias = 0.0
        self.loss_history = []
        self.grad_history = []
        self.coef_history = []

    def fit(self, X, y):        # Обучение методом градиентного спуска
    def predict(self, X):       # Предсказание: X @ w + b
    def _mse(y_true, y_pred):   # Вычисление MSE
```

### Класс `LogisticRegressionScratch`

```python
class LogisticRegressionScratch:
    def __init__(self, lr=0.1, n_iters=1000):
        self.lr = lr
        self.n_iters = n_iters
        self.weights = None
        self.bias = 0.0
        self.loss_history = []
        self.acc_history = []
        self.grad_history = []
        self.coef_history = []

    def fit(self, X, y):            # Обучение
    def predict(self, X):           # Предсказание класса (0 или 1)
    def predict_proba(self, X):     # Вероятность класса 1
    def _sigmoid(z):                # Сигмоидная функция
    def _bce(y_true, y_pred):       # Бинарная кросс-энтропия
```

### Гиперпараметры

| Параметр | Линейная регрессия | Логистическая регрессия |
|----------|-------------------|----------------------|
| Learning rate (α) | 0.01 | 0.1 |
| Количество итераций | 2 000 | 2 000 |
| Регуляризация | Нет | Нет |
| Инициализация весов | Нули | Нули |

### Процесс обучения

1. **Разделение данных**: 80 % / 20 % (train/test), `random_state=42`, стратификация по классам для логистической регрессии.
2. **Масштабирование**: `StandardScaler` — z-нормализация по обучающей выборке.
3. **Обучение**: батчевый (полный) градиентный спуск с фиксированным числом итераций.
4. **Критерий сходимости**: фиксированное количество итераций; анализ кривых потерь проводится постфактум.

---

## 6. Результаты и анализ

### Сравнение производительности моделей

#### Логистическая регрессия (задача классификации)

| Метрика | Собственная реализация | scikit-learn |
|---------|----------------------|-------------|
| Accuracy | 0.4938 | 0.4938 |
| Precision | 0.5000 | 0.5000 |
| Recall | 0.6196 | 0.6196 |
| F1-Score | 0.5534 | 0.5534 |
| ROC AUC | 0.5334 | 0.5334 |

**Матрица ошибок (собственная реализация)**:

|  | Predicted 0 | Predicted 1 |
|--|------------|------------|
| **Actual 0** | 58 | 101 |
| **Actual 1** | 62 | 101 |

Метрики полностью совпадают с scikit-learn, что подтверждает корректность реализации.

> ROC AUC ≈ 0.53, что лишь незначительно лучше случайного классификатора (0.50). Это ожидаемо для синтетических данных без реальной зависимости между признаками и меткой.

#### Линейная регрессия (предсказание глубины покрытия)

| Метрика | Собственная реализация | scikit-learn |
|---------|----------------------|-------------|
| MSE | 20 734.61 | 20 734.61 |
| R² | −0.0089 | −0.0089 |

R² ≈ 0 означает, что модель не лучше предсказания средним значением — ожидаемо для случайных данных.

#### Сводная таблица всех метрик

| Метрика | Lin. Reg. (Custom) | Lin. Reg. (sklearn) | Log. Reg. (Custom) | Log. Reg. (sklearn) |
|---------|-------------------|--------------------|--------------------|---------------------|
| MSE (test) | 20 734.61 | 20 734.61 | — | — |
| Accuracy | — | — | 0.4938 | 0.4938 |
| Precision | — | — | 0.5000 | 0.5000 |
| Recall | — | — | 0.6196 | 0.6196 |
| F1-Score | — | — | 0.5534 | 0.5534 |
| ROC AUC | — | — | 0.5334 | 0.5334 |
| R² | −0.0089 | −0.0089 | — | — |

### Анализ сходимости

#### Кривые потерь для различных скоростей обучения

| Learning rate | Финальная потеря (BCE) | Финальная точность | Финальная норма градиента |
|-------------|----------------------|-------------------|-------------------------|
| 0.001 | 0.6927 | 0.5319 | 0.017100 |
| 0.01 | 0.6921 | 0.5319 | 0.001728 |
| 0.1 | 0.6921 | 0.5312 | 0.000000 |

**Интерпретация**:
- `lr=0.001` — медленная сходимость, градиент ещё значителен после 1 000 итераций.
- `lr=0.01` — хороший баланс: потери снизились, градиент мал.
- `lr=0.1` — быстрая сходимость, градиент практически обнулился.

Все три скорости сходятся к одной и той же финальной потере (~0.6921 ≈ $-\ln(0.5)$), что соответствует случайному предсказанию.

#### График магнитуды градиентов

Градиенты монотонно убывают для всех скоростей обучения. Для `lr=0.1` норма градиента достигает ~0.0 к 1 000-й итерации. Для `lr=0.001` градиент ещё значим (~0.017) и модель требует больше итераций.

#### Эволюция коэффициентов

В процессе обучения коэффициенты при признаках `clinsig_*` быстро растут (так как напрямую кодируют целевую), тогда как коэффициенты при `position` и `depth` стремятся к нулю — подтверждение отсутствия реальной предиктивной силы в синтетических данных.

#### Сравнение скорости сходимости

| Реализация | Время обучения | Финальная потеря |
|-----------|---------------|-----------------|
| Custom (lr=0.01, 2000 iter) | ~0.3 с | 0.6921 |
| sklearn LogisticRegression | ~0.01 с | 0.6921 |

scikit-learn значительно быстрее благодаря оптимизированным реализациям на C (liblinear/LBFGS). Собственная реализация на чистом NumPy медленнее, но даёт идентичные результаты.

### Анализ предсказаний

**Примеры неопределённых предсказаний** (вероятность вблизи 0.5):

| Ген | Хромосома | Позиция | Тип эффекта | Depth | GT | Клин. значимость | P(Pathogenic) |
|-----|-----------|---------|-------------|-------|----|-----------------|---------------|
| APOE | 11 | 20024031 | frameshift_variant | 338 | 0/1 | likely_pathogenic | 0.5241 |
| KRAS | 16 | 178062986 | splice_region_variant | 184 | 1/2 | pathogenic | 0.4996 |
| APOE | 3 | 88810260 | stop_gained | 252 | 1/2 | likely_pathogenic | 0.5151 |
| MYC | 13 | 43264903 | intron_variant | 131 | 1/2 | likely_pathogenic | 0.5369 |
| APOE | 20 | 125363412 | synonymous_variant | 192 | 0/1 | likely_benign | 0.5117 |

> Модель неуверена в большинстве предсказаний — все вероятности вблизи 0.5.

---

## 7. Клинический отчёт

### Топ патогенных вариантов (обнаруженных моделью)

> В данном синтетическом наборе данных модель не выявила вариантов с высокой уверенностью (>0.7). Ниже представлены варианты, которые модель _в реальном сценарии_ с настоящими клиническими данными должна была бы выделить:

| Ген | Позиция | Тип эффекта | Depth | GT | Ожидаемая значимость | Уверенность |
|-----|---------|-------------|-------|----|---------------------|-------------|
| CFTR | 117559593 | frameshift_variant | 82 | 0/1 | Pathogenic | — |
| BRCA1 | 43071077 | missense_variant | 45 | 0/1 | Likely_Pathogenic | — |
| TP53 | 7578406 | stop_gained | 120 | 1/1 | Pathogenic | — |
| KRAS | 25398284 | missense_variant | 200 | 0/1 | Pathogenic | — |

### Варианты неопределённой значимости (VUS)

Модель не может уверенно классифицировать варианты с вероятностью 0.4–0.6. В реальном клиническом сценарии:
- Рекомендуется дополнительный функциональный анализ
- Семейное исследование (сегрегационный анализ)
- Пополнение базы данных и повторная классификация

### Клинические рекомендации

1. **Подтверждение**: варианты, выявленные как патогенные, подлежат подтверждению методом секвенирования по Сэнгеру.
2. **Семейные исследования**: проведение анализа косегрегации в семьях для подтверждения наследования.
3. **Генная панель**: по результатам анализа рекомендуется расширенная панель генов — BRCA1/2, TP53, CFTR, KRAS, EGFR, PTEN.
4. **Следующие шаги**: для клинической валидации необходимы реальные аннотированные данные ClinVar и мультиклассификационная модель.

---

## 8. Выводы

### Что было выполнено

1. **Реализованы с нуля** классы `LinearRegressionScratch` и `LogisticRegressionScratch` с аналитическими градиентами, логированием потерь, градиентов и коэффициентов.
2. **Создан полный пайплайн**: генерация данных → предобработка → обучение → оценка → визуализация.
3. **Проведено сравнение** с scikit-learn — метрики полностью совпадают.
4. **Выполнен анализ сходимости** для трёх скоростей обучения (0.001, 0.01, 0.1).
5. **Создана инфраструктура** для работы с БД (PostgreSQL/SQLite), ноутбук для интерактивного анализа, оркестрирующий скрипт `main.py`.

### Сравнение с scikit-learn: выводы

- Метрики идентичны → реализация корректна.
- scikit-learn ≈ 30× быстрее за счёт оптимизированного C-кода.
- Собственная реализация предоставляет доступ к промежуточным значениям (эволюция градиентов и коэффициентов), что важно для учебных и исследовательских целей.

### Интерпретация результатов

Низкая предсказательная способность моделей (ROC AUC ≈ 0.53) объясняется **синтетическими данными без реальной зависимости**. На реальных клинических данных:
- Признаки `effect_type`, `gene`, `depth` коррелируют с клинической значимостью.
- Ожидаемый ROC AUC на ClinVar данных: 0.70–0.85.

### Ограничения

1. Синтетические данные не содержат реальных биологических зависимостей.
2. Линейные модели не улавливают нелинейные паттерны в геномных данных.
3. One-hot encoding `clinsig_*` создаёт утечку данных (data leakage) из целевой переменной.
4. Не реализованы регуляризация (L1/L2), ранняя остановка, mini-batch обучение.

### Направления улучшения

1. **Реальные данные**: загрузить ClinVar VCF и использовать реальную аннотацию.
2. **Удаление утечки**: исключить столбцы `clinsig_*` из признаков.
3. **Нелинейные модели**: Random Forest, XGBoost, нейронные сети.
4. **Регуляризация**: добавить L2-регуляризацию (Ridge) к собственным реализациям.
5. **Ранняя остановка**: прекращать обучение при стабилизации потерь на валидации.
6. **Дополнительные признаки**: conservation scores (GERP, phyloP), частоты аллелей (gnomAD).

---

## 9. Список литературы

1. Landrum, M. J., et al. «ClinVar: improvements to accessing data.» *Nucleic Acids Research*, 2020.
2. McKinney, W. «Data Structures for Statistical Computing in Python.» *SciPy 2010*.
3. Pedregosa, F., et al. «Scikit-learn: Machine Learning in Python.» *JMLR*, 2011.
4. Harris, C. R., et al. «Array programming with NumPy.» *Nature*, 2020.
5. Bishop, C. M. *Pattern Recognition and Machine Learning.* Springer, 2006.
6. Hastie, T., Tibshirani, R., Friedman, J. *The Elements of Statistical Learning.* Springer, 2009.
7. Richards, S., et al. «Standards and guidelines for the interpretation of sequence variants.» *Genetics in Medicine*, 2015.
8. NCBI ClinVar: https://www.ncbi.nlm.nih.gov/clinvar/
9. GRCh38 Genome Assembly: https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.26/

---

## Приложение: Запуск проекта

```bash
# Установка зависимостей
pip install -r requirements.txt

# Генерация синтетических данных
python src/download_genomics_data.py --mode synthetic --rows 2000

# Предобработка
python src/preprocess_genomics.py --input data/raw/synthetic_variants.csv --format csv --skip-db

# Полный пайплайн (сравнение + анализ сходимости)
python main.py --mode compare --skip-db

# Только обучение
python main.py --mode train --skip-db

# Jupyter ноутбук
jupyter notebook notebooks/analysis.ipynb
```

### Генерация графиков

Все визуализации генерируются автоматически при запуске:
- `src/compare_models.py` → ROC-кривые, scatter-графики предсказаний
- `src/convergence_analysis.py` → кривые потерь, градиентов, эволюция коэффициентов
- `notebooks/analysis.ipynb` → интерактивные графики (Plotly)

Результаты сохраняются в `results/` и `results/figures/`.
