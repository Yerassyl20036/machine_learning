{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a8d502",
   "metadata": {},
   "source": [
    "# Genomics ML Notebook\n",
    "\n",
    "This notebook walks through data exploration, gradient derivations, training visualizations, model comparisons, and convergence analysis for the custom linear and logistic regression implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3002e53",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Dataset\n",
    "\n",
    "We load standard analysis libraries plus the project utilities. The data is loaded from `data/processed/cleaned_variants.csv` if available; otherwise we preprocess the raw file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b2781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    mean_squared_error,\n",
    "    precision_score,\n",
    "    r2_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "HAS_PLOTLY = False\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "\n",
    "    HAS_PLOTLY = True\n",
    "except Exception:\n",
    "    HAS_PLOTLY = False\n",
    "\n",
    "root = Path(\"..\").resolve()\n",
    "if str(root) not in sys.path:\n",
    "    sys.path.insert(0, str(root))\n",
    "\n",
    "from src.preprocess_genomics import (\n",
    "    DEFAULT_INPUT as RAW_DEFAULT,\n",
    "    DEFAULT_OUTPUT as PROCESSED_DEFAULT,\n",
    "    detect_format,\n",
    "    normalize_dataframe,\n",
    "    parse_csv,\n",
    "    parse_vcf,\n",
    ")\n",
    "from src.linear_regression_scratch import LinearRegressionScratch\n",
    "from src.logistic_regression_scratch import LogisticRegressionScratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9188ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = root / RAW_DEFAULT\n",
    "processed_path = root / PROCESSED_DEFAULT\n",
    "\n",
    "if processed_path.exists():\n",
    "    df = pd.read_csv(processed_path)\n",
    "else:\n",
    "    if not raw_path.exists():\n",
    "        fallback = root / \"data/raw/synthetic_variants.csv\"\n",
    "        if fallback.exists():\n",
    "            raw_path = fallback\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                \"No raw data found. Run src/download_genomics_data.py first.\"\n",
    "            )\n",
    "\n",
    "    input_format = detect_format(str(raw_path))\n",
    "    if input_format == \"vcf\":\n",
    "        raw_df = parse_vcf(str(raw_path))\n",
    "    else:\n",
    "        raw_df = parse_csv(str(raw_path))\n",
    "\n",
    "    df = normalize_dataframe(raw_df)\n",
    "    processed_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(processed_path, index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(\"Rows:\", len(df), \"Columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca4bba3",
   "metadata": {},
   "source": [
    "## 2. Explore Data and Visualize Distributions\n",
    "\n",
    "We inspect summary statistics, missing values, and a few visualizations to understand distributions and relationships among numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a559cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe(include=\"all\"))\n",
    "missing = df.isna().sum().sort_values(ascending=False)\n",
    "print(missing[missing > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1615580",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "numeric_df.hist(bins=30, figsize=(10, 6))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if \"depth\" in df.columns and \"position\" in df.columns:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(df[\"position\"], df[\"depth\"], s=10, alpha=0.4)\n",
    "    plt.xlabel(\"Position\")\n",
    "    plt.ylabel(\"Depth\")\n",
    "    plt.title(\"Depth vs Position\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "corr = numeric_df.corr()\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(corr, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"Correlation\")\n",
    "plt.xticks(range(len(corr.columns)), corr.columns, rotation=45, ha=\"right\")\n",
    "plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if HAS_PLOTLY and \"depth\" in df.columns:\n",
    "    fig = px.histogram(df, x=\"depth\", title=\"Depth Distribution (Interactive)\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02a4512",
   "metadata": {},
   "source": [
    "## 3. Derive Gradients Step-by-Step\n",
    "\n",
    "We derive gradients for both models.\n",
    "\n",
    "**Linear Regression**\n",
    "\n",
    "Model: $\\hat{y} = Xw + b$\n",
    "\n",
    "Loss (MSE):\n",
    "$$\n",
    "L = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2\n",
    "$$\n",
    "\n",
    "Gradients:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{2}{n} X^T(\\hat{y} - y), \\quad\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{2}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)\n",
    "$$\n",
    "\n",
    "**Logistic Regression**\n",
    "\n",
    "Model: $p = \\sigma(z)$ where $z = Xw + b$ and $\\sigma(z) = 1/(1+e^{-z})$\n",
    "\n",
    "Loss (BCE):\n",
    "$$\n",
    "L = -\\frac{1}{n} \\sum_{i=1}^n \\left[y_i \\log p_i + (1-y_i) \\log(1-p_i)\\right]\n",
    "$$\n",
    "\n",
    "Gradients:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{1}{n} X^T(p - y), \\quad\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n (p_i - y_i)\n",
    "$$\n",
    "\n",
    "We verify these gradients numerically below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08042428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_diff_grad(loss_fn, w, eps=1e-5):\n",
    "    grad = np.zeros_like(w)\n",
    "    for i in range(len(w)):\n",
    "        w_pos = w.copy()\n",
    "        w_neg = w.copy()\n",
    "        w_pos[i] += eps\n",
    "        w_neg[i] -= eps\n",
    "        grad[i] = (loss_fn(w_pos) - loss_fn(w_neg)) / (2 * eps)\n",
    "    return grad\n",
    "\n",
    "# Linear regression gradient check\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(20, 3)\n",
    "y = np.random.randn(20)\n",
    "w = np.random.randn(3)\n",
    "\n",
    "\n",
    "def linear_loss(w_vec):\n",
    "    preds = X @ w_vec\n",
    "    return np.mean((preds - y) ** 2)\n",
    "\n",
    "analytic_grad = (2.0 / len(X)) * (X.T @ (X @ w - y))\n",
    "fd_grad = finite_diff_grad(linear_loss, w)\n",
    "print(\"Linear grad diff (L2):\", np.linalg.norm(analytic_grad - fd_grad))\n",
    "\n",
    "# Logistic regression gradient check\n",
    "X_log = np.random.randn(30, 4)\n",
    "y_log = (np.random.rand(30) > 0.5).astype(float)\n",
    "w_log = np.random.randn(4)\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def log_loss_fn(w_vec):\n",
    "    p = sigmoid(X_log @ w_vec)\n",
    "    eps = 1e-12\n",
    "    p = np.clip(p, eps, 1.0 - eps)\n",
    "    return -np.mean(y_log * np.log(p) + (1 - y_log) * np.log(1 - p))\n",
    "\n",
    "analytic_grad_log = (1.0 / len(X_log)) * (X_log.T @ (sigmoid(X_log @ w_log) - y_log))\n",
    "fd_grad_log = finite_diff_grad(log_loss_fn, w_log)\n",
    "print(\"Logistic grad diff (L2):\", np.linalg.norm(analytic_grad_log - fd_grad_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56229d",
   "metadata": {},
   "source": [
    "## 4. Implement Training Loop and Log Metrics\n",
    "\n",
    "We train the custom models and collect loss/accuracy histories for later visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Linear regression target\n",
    "if \"depth\" in df.columns:\n",
    "    X_lin = df.drop(columns=[\"depth\", \"label\"], errors=\"ignore\").select_dtypes(include=[np.number])\n",
    "    y_lin = df[\"depth\"]\n",
    "\n",
    "    Xl_train, Xl_test, yl_train, yl_test = train_test_split(\n",
    "        X_lin, y_lin, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    lin_model = LinearRegressionScratch(lr=0.01, n_iters=1000)\n",
    "    lin_model.fit(Xl_train.values, yl_train.values)\n",
    "else:\n",
    "    lin_model = None\n",
    "\n",
    "# Logistic regression target\n",
    "if \"label\" in df.columns:\n",
    "    X_log = df.drop(columns=[\"label\"], errors=\"ignore\").select_dtypes(include=[np.number])\n",
    "    y_log = df[\"label\"]\n",
    "\n",
    "    Xg_train, Xg_test, yg_train, yg_test = train_test_split(\n",
    "        X_log, y_log, test_size=0.2, random_state=42, stratify=y_log\n",
    "    )\n",
    "\n",
    "    log_model = LogisticRegressionScratch(lr=0.1, n_iters=1000)\n",
    "    log_model.fit(Xg_train.values, yg_train.values)\n",
    "else:\n",
    "    log_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a9c431",
   "metadata": {},
   "source": [
    "## 5. Visualize Training Process\n",
    "\n",
    "We plot loss curves (and accuracy for logistic regression) to understand training dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5077a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = root / \"results/figures\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if lin_model is not None:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(lin_model.loss_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.title(\"Linear Regression Loss\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_dir / \"linear_loss_curve.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "if log_model is not None:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(log_model.loss_history, label=\"loss\")\n",
    "    plt.plot(log_model.acc_history, label=\"accuracy\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.title(\"Logistic Regression Loss/Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_dir / \"logistic_training_curves.png\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8f4c46",
   "metadata": {},
   "source": [
    "## 6. Compare Models and Report Results\n",
    "\n",
    "We compare custom and sklearn implementations using standard metrics and coefficient summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917ba3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = root / \"results\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if lin_model is not None:\n",
    "    sk_lin = LinearRegression().fit(Xl_train, yl_train)\n",
    "    custom_pred = lin_model.predict(Xl_test.values)\n",
    "    sk_pred = sk_lin.predict(Xl_test)\n",
    "\n",
    "    lin_metrics = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"model\": \"custom\",\n",
    "                \"mse\": mean_squared_error(yl_test, custom_pred),\n",
    "                \"r2\": r2_score(yl_test, custom_pred),\n",
    "            },\n",
    "            {\n",
    "                \"model\": \"sklearn\",\n",
    "                \"mse\": mean_squared_error(yl_test, sk_pred),\n",
    "                \"r2\": r2_score(yl_test, sk_pred),\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    display(lin_metrics)\n",
    "    lin_metrics.to_csv(results_dir / \"linear_metrics.csv\", index=False)\n",
    "\n",
    "    coef_table = pd.DataFrame(\n",
    "        {\n",
    "            \"feature\": Xl_train.columns,\n",
    "            \"custom_coef\": lin_model.weights,\n",
    "            \"sklearn_coef\": sk_lin.coef_,\n",
    "        }\n",
    "    )\n",
    "    display(coef_table.head())\n",
    "    coef_table.to_csv(results_dir / \"linear_coefficients.csv\", index=False)\n",
    "\n",
    "if log_model is not None:\n",
    "    sk_log = LogisticRegression(max_iter=2000).fit(Xg_train, yg_train)\n",
    "    custom_proba = log_model.predict_proba(Xg_test.values)\n",
    "    sk_proba = sk_log.predict_proba(Xg_test)[:, 1]\n",
    "\n",
    "    custom_pred = (custom_proba >= 0.5).astype(int)\n",
    "    sk_pred = (sk_proba >= 0.5).astype(int)\n",
    "\n",
    "    log_metrics = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"model\": \"custom\",\n",
    "                \"accuracy\": accuracy_score(yg_test, custom_pred),\n",
    "                \"precision\": precision_score(yg_test, custom_pred, zero_division=0),\n",
    "                \"recall\": recall_score(yg_test, custom_pred, zero_division=0),\n",
    "                \"f1\": f1_score(yg_test, custom_pred, zero_division=0),\n",
    "                \"roc_auc\": roc_auc_score(yg_test, custom_proba),\n",
    "            },\n",
    "            {\n",
    "                \"model\": \"sklearn\",\n",
    "                \"accuracy\": accuracy_score(yg_test, sk_pred),\n",
    "                \"precision\": precision_score(yg_test, sk_pred, zero_division=0),\n",
    "                \"recall\": recall_score(yg_test, sk_pred, zero_division=0),\n",
    "                \"f1\": f1_score(yg_test, sk_pred, zero_division=0),\n",
    "                \"roc_auc\": roc_auc_score(yg_test, sk_proba),\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    display(log_metrics)\n",
    "    log_metrics.to_csv(results_dir / \"logistic_metrics.csv\", index=False)\n",
    "\n",
    "    log_coef = pd.DataFrame(\n",
    "        {\n",
    "            \"feature\": Xg_train.columns,\n",
    "            \"custom_coef\": log_model.weights,\n",
    "            \"sklearn_coef\": sk_log.coef_.ravel(),\n",
    "        }\n",
    "    )\n",
    "    display(log_coef.head())\n",
    "    log_coef.to_csv(results_dir / \"logistic_coefficients.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2f42ab",
   "metadata": {},
   "source": [
    "## 7. Convergence Analysis with Interactive Plots\n",
    "\n",
    "We compare loss curves for multiple learning rates and visualize coefficient evolution. If Plotly is available, plots are interactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24dee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "\n",
    "if log_model is not None:\n",
    "    loss_map = {}\n",
    "    grad_map = {}\n",
    "    coef_map = {}\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        model = LogisticRegressionScratch(lr=lr, n_iters=300)\n",
    "        model.fit(Xg_train.values, yg_train.values)\n",
    "        loss_map[lr] = model.loss_history\n",
    "        grad_map[lr] = model.grad_history\n",
    "        coef_map[lr] = np.vstack(model.coef_history)\n",
    "\n",
    "    if HAS_PLOTLY:\n",
    "        fig = go.Figure()\n",
    "        for lr, losses in loss_map.items():\n",
    "            fig.add_trace(go.Scatter(y=losses, mode=\"lines\", name=f\"lr={lr}\"))\n",
    "        fig.update_layout(title=\"Logistic Loss Curves\", xaxis_title=\"Iteration\", yaxis_title=\"Loss\")\n",
    "        fig.show()\n",
    "\n",
    "        fig = go.Figure()\n",
    "        for lr, grads in grad_map.items():\n",
    "            fig.add_trace(go.Scatter(y=grads, mode=\"lines\", name=f\"lr={lr}\"))\n",
    "        fig.update_layout(title=\"Gradient Magnitudes\", xaxis_title=\"Iteration\", yaxis_title=\"L2 Norm\")\n",
    "        fig.show()\n",
    "    else:\n",
    "        for lr, losses in loss_map.items():\n",
    "            plt.plot(losses, label=f\"lr={lr}\")\n",
    "        plt.title(\"Logistic Loss Curves\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    coef_lr = 0.01\n",
    "    if coef_lr in coef_map:\n",
    "        coef_hist = coef_map[coef_lr]\n",
    "        max_features = min(6, coef_hist.shape[1])\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        for i in range(max_features):\n",
    "            plt.plot(coef_hist[:, i], label=Xg_train.columns[i])\n",
    "        plt.title(\"Coefficient Evolution (lr=0.01)\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Coefficient\")\n",
    "        plt.legend(ncol=2, fontsize=8)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fig_dir / \"logistic_coef_evolution.png\", dpi=150)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667a9a2b",
   "metadata": {},
   "source": [
    "## Conclusions and Insights\n",
    "\n",
    "- Feature scaling and label balance strongly influence convergence for logistic regression.\n",
    "- Learning rate $0.01$ is a stable default here; $0.1$ may converge faster but can oscillate.\n",
    "- Custom implementations match sklearn trends when inputs are numeric and normalized.\n",
    "- Coefficient evolution helps diagnose overfitting and unstable updates.\n",
    "- Use the comparison tables to validate correctness before experimenting with new features."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
