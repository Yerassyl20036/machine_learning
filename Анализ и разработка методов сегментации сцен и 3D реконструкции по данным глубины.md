# Анализ и разработка методов сегментации сцен и 3D реконструкции по данным глубины

## 1. Введение

Автоматическая сегментация сцен и трёхмерная реконструкция являются ключевыми задачами компьютерного зрения с широким применением в робототехнике, дополненной реальности и автономной навигации. Развитие RGB-D сенсоров (Microsoft Kinect, Intel RealSense, Apple LiDAR) обеспечивает доступ к синхронизированным RGB-изображениям и картам глубины в реальном времени [1].

Современные подходы к сегментации основаны на свёрточных нейронных сетях (U-Net [2], SegNet [3], DeepLab [4]), а методы реконструкции включают TSDF [5], KinectFusion [6] и нейронные представления сцен (NeRF [7]). Однако state-of-the-art решения требуют значительных вычислительных ресурсов, что ограничивает их применение в условиях ограниченных ресурсов [8].

Цель работы — исследование базовых методов сегментации и реконструкции на датасете NYU Depth V2, создание baseline-системы, работающей на CPU, и сравнительный анализ с современными подходами.

---

## 2. Обзор литературы

Задача семантической сегментации формулируется как попиксельная классификация изображения на C классов. Современные архитектуры основаны на encoder-decoder структуре с skip connections [2]. U-Net использует симметричный энкодер-декодер для сохранения пространственной информации, SegNet применяет pooling indices для upsampling [3], DeepLab v3+ использует Atrous Spatial Pyramid Pooling для захвата мультимасштабного контекста [4].

Трёхмерная реконструкция из RGB-D данных основана на интеграции нескольких кадров в объёмную модель. TSDF представляет сцену как регулярную 3D сетку вокселей с signed distance до ближайшей поверхности [5]. KinectFusion реализует TSDF в реальном времени с использованием GPU и ICP для выравнивания кадров [6]. BundleFusion расширяет подход с использованием sparse keyframe-based SLAM [9].

NYU Depth Dataset V2 [10] является стандартным бенчмарком, содержащим 1449 аннотированных кадров из 464 внутренних сцен с 40 классами объектов. ScanNet [11] предоставляет 2.5 млн кадров из 1513 сцен. Сравнительный анализ современных методов на NYU V2 показывает, что FCN достигает mIoU 29.2%, SegNet — 31.8%, DeepLab v3+ — 45.2%, RefineNet — 46.5% [4,12,13].

Нейронные методы представления сцен включают NeRF [7], представляющий сцену как непрерывную функцию, Instant-NGP [14] с multiresolution hash encoding для быстрого обучения, и NICE-SLAM [15], комбинирующий implicit representations с SLAM. Классические методы остаются актуальными для real-time приложений благодаря предсказуемой производительности.

---

## 3. Методология

В работе использован датасет NYU Depth Dataset V2 [10], содержащий 1449 плотно аннотированных RGB-D кадров (640×480 пикселей) из внутренних помещений с 40 классами объектов. Создано сбалансированное подмножество: 240 кадров для обучения (60%), 160 для тестирования (40%). Предобработка включала нормализацию RGB (StandardScaler), линейное масштабирование карт глубины к [0,1] и экспорт в формат PNG/NumPy.

Для сегментации разработана архитектура Tiny U-Net — упрощённый вариант классической U-Net [2] с 3 уровнями (16-32-64 фильтров) и skip connections между энкодером и декодером. Модель содержит ~50K параметров, обучалась 5 эпох с оптимизатором Adam (lr=1e-3, batch=4) и функцией потерь CrossEntropyLoss на CPU. Метрики качества: mean IoU, pixel accuracy, precision, recall, F1-score.

Для реконструкции применён метод TSDF fusion [5] с использованием библиотеки Open3D [16]. Параметры: voxel size 0.02 м, truncation distance 0.04 м. Pipeline включает создание облака точек из RGB-D, интеграцию в TSDF volume, извлечение mesh алгоритмом Marching Cubes и пост-обработку (statistical outlier removal, Gaussian filtering). Калибровка камеры Kinect v1: fx=fy=525.0, cx=319.5, cy=239.5. Метрики: RMSE, AbsRel, threshold accuracies.

---

## 4. Результаты и обсуждение

Обучение Tiny U-Net на 240 изображениях показало плавное снижение loss без overfitting. Финальные метрики на тестовой выборке (160 кадров): mean IoU 0.22%, pixel accuracy 27.98%, precision 8.14%, recall 7.92%, F1-score 7.81%. Низкие значения обусловлены малой ёмкостью модели (50K параметров) и сильным дисбалансом классов — модель частично распознаёт доминантные классы (wall, floor), но игнорирует редкие объекты.

TSDF реконструкция на 50 тестовых кадрах показала высокую точность: RMSE 0.0292 м (2.92 см), AbsRel 0.33%, threshold accuracy δ<1.25 = 99.99%. Применённые фильтры (Gaussian smoothing, statistical outlier removal) эффективно устраняют шум Kinect сенсора, обеспечивая детальное восстановление геометрии помещений.

Сравнительный анализ с state-of-the-art методами показал, что baseline сегментация уступает современным подходам в 200+ раз по mIoU (FCN 29.2%, SegNet 31.8%, DeepLab v3+ 45.2%, RefineNet 46.5% vs 0.22%), но имеет в 100+ раз меньше параметров и работает на CPU [4,12,13]. Реконструкция продемонстрировала конкурентное качество, превзойдя KinectFusion по RMSE (0.045 м vs 0.029 м) благодаря пост-обработке [6].

Время выполнения на Apple M1 CPU: обучение сегментации 108 минут (5 эпох), inference 1.2 сек/кадр, TSDF реконструкция 14.8 сек/кадр. Real-time производительность (30 FPS) недостижима на CPU, требуется GPU-оптимизация.

---

## 5. Заключение

В работе проведено экспериментальное исследование базовых методов семантической сегментации и трёхмерной реконструкции на датасете NYU Depth V2. Разработана воспроизводимая baseline-система, работающая на CPU без требований к GPU. Получены количественные оценки: mIoU 0.22%, pixel accuracy 27.98% для сегментации, RMSE 0.029 м для реконструкции.

Сравнительный анализ показал, что baseline сегментация уступает state-of-the-art методам (DeepLab v3+ 45.2%, RefineNet 46.5%) из-за малой ёмкости модели и дисбаланса классов [4,13]. Направления улучшения включают увеличение глубины U-Net, применение weighted loss для балансировки классов, интеграцию ASPP модулей для мультимасштабного контекста и multi-modal архитектур (RGB+Depth fusion).

Реконструкция продемонстрировала конкурентное качество, превзойдя KinectFusion по точности [6]. Для достижения real-time производительности необходима GPU-реализация и интеграция SLAM для multi-frame consistency. Дальнейшие исследования направлены на применение attention mechanisms [17], нейронных implicit representations (NeRF, Instant-NGP) [7,14] и cross-dataset обучение для повышения обобщающей способности.

Практическая значимость работы заключается в создании доступной baseline-системы для образовательных целей, прототипирования алгоритмов и edge computing приложений. Модульная архитектура позволяет легко заменять компоненты и проводить сравнительный анализ с более сложными методами.

---

## Список литературы

[1] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, "Indoor Segmentation and Support Inference from RGBD Images," *ECCV*, 2012.

[2] O. Ronneberger, P. Fischer, and T. Brox, "U-Net: Convolutional Networks for Biomedical Image Segmentation," *MICCAI*, 2015.

[3] V. Badrinarayanan, A. Kendall, and R. Cipolla, "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation," *IEEE Trans. PAMI*, vol. 39, no. 12, pp. 2481–2495, 2017.

[4] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation," *ECCV*, 2018.

[5] B. Curless and M. Levoy, "A volumetric method for building complex models from range images," *SIGGRAPH*, 1996.

[6] R. A. Newcombe et al., "KinectFusion: Real-time dense surface mapping and tracking," *ISMAR*, 2011.

[7] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis," *ECCV*, 2020.

[8] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove, "DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation," *CVPR*, 2019.

[9] A. Dai, M. Nießner, M. Zollhöfer, S. Izadi, and C. Theobalt, "BundleFusion: Real-time Globally Consistent 3D Reconstruction using Online Surface Re-integration," *ACM Trans. Graphics*, vol. 36, no. 3, 2017.

[10] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, "Indoor Segmentation and Support Inference from RGBD Images," *ECCV*, 2012.

[11] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner, "ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes," *CVPR*, 2017.

[12] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, "Pyramid Scene Parsing Network," *CVPR*, 2017.

[13] G. Lin, A. Milan, C. Shen, and I. Reid, "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation," *CVPR*, 2017.

[14] T. Müller, A. Evans, C. Schied, and A. Keller, "Instant Neural Graphics Primitives with a Multiresolution Hash Encoding," *ACM Trans. Graphics (SIGGRAPH)*, vol. 41, no. 4, 2022.

[15] Z. Zhu, S. Peng, V. Larsson, W. Xu, H. Bao, Z. Cui, M. R. Oswald, and M. Pollefeys, "NICE-SLAM: Neural Implicit Scalable Encoding for SLAM," *CVPR*, 2022.

[16] Q.-Y. Zhou, J. Park, and V. Koltun, "Open3D: A Modern Library for 3D Data Processing," *arXiv:1801.09847*, 2018.

[17] A. Vaswani et al., "Attention Is All You Need," *NeurIPS*, 2017.
